# ğŸ“„ RAG Chat

A **Retrieval-Augmented Generation (RAG)** chat application that enables users to upload PDF documents and ask questions. Answers are generated exclusively from the document content, with transparent source references for full traceability.

---

## âœ¨ Features

- **PDF Document Upload** â€“ Seamlessly upload PDF files for processing
- **Automatic Chunking & Embedding** â€“ Documents are intelligently split and vectorized
- **Vector Search with ChromaDB** â€“ Fast and accurate semantic search across document chunks
- **Context-Only Answers** â€“ Powered by Google Gemini API, ensuring responses are grounded in your documents
- **Source References** â€“ Every answer includes the exact chunks used, displayed for verification
- **Smart Chat Control** â€“ Chat interface is disabled until a document is successfully ingested

---

## ğŸ—ï¸ Tech Stack

### Backend

- **Node.js** + **Express** â€“ Server framework
- **Google Gemini API** â€“ Language model for answer generation
- **ChromaDB** â€“ Vector database for embeddings
- **Docker** & **Docker Compose** â€“ Containerized deployment

### Frontend

- **React** â€“ UI framework
- **Vite** â€“ Fast development and build tooling
- **Fetch API** â€“ HTTP requests
- **CSS** â€“ Styling

---

## ğŸ“ Project Structure

```
rag-chat/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ ingest/          # PDF processing and chunking logic
â”‚   â”‚   â”œâ”€â”€ query/            # Query handling and RAG pipeline
â”‚   â”‚   â”œâ”€â”€ vectorstore/      # ChromaDB integration
â”‚   â”‚   â””â”€â”€ server.js         # Express server entry point
â”‚   â”œâ”€â”€ docker-compose.yml    # Docker services configuration
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env                  # Environment variables
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ components/       # React components
    â”‚   â”œâ”€â”€ App.jsx           # Main app component
    â”‚   â””â”€â”€ api.js            # API service layer
    â”œâ”€â”€ package.json
    â””â”€â”€ vite.config.js
```

---

## ğŸ”Œ API Endpoints

### Ingest PDF

**POST** `/api/ingest`

Upload and process a PDF document.

**Request:**

- Content-Type: `multipart/form-data`
- Body: `file` (PDF file)

**Response:**

```json
{
  "message": "File uploaded successfully",
  "chunks": 36
}
```

### Query Document

**POST** `/api/query`

Ask questions about the ingested document.

**Request:**

```json
{
  "query": "What is the main topic of this document?"
}
```

**Response:**

```json
{
  "answer": "The main topic is...",
  "sources": ["chunk text 1 from page 3", "chunk text 2 from page 7"]
}
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** (v18+)
- **Docker** & **Docker Compose**
- **Google Gemini API Key** ([Get one here](https://ai.google.dev/))

### ğŸ³ Backend Setup

1. Navigate to the backend directory:

```bash
   cd backend
```

2. Create a `.env` file:

```env
   GEMINI_API_KEY=your_api_key_here
   CHROMA_HOST=chromadb
```

3. Start the services:

```bash
   docker compose up --build
```

4. Services will be available at:
   - **Backend API:** http://localhost:5000
   - **ChromaDB:** http://localhost:8000

### ğŸ¨ Frontend Setup

1. Navigate to the frontend directory:

```bash
   cd frontend
```

2. Install dependencies:

```bash
   npm install
```

3. Start the development server:

```bash
   npm run dev
```

4. Open your browser:
   - **Frontend:** http://localhost:5173

---

## ğŸ§  How It Works

1. **Document Ingestion** â€“ PDFs are uploaded, parsed, and split into semantic chunks
2. **Vectorization** â€“ Each chunk is embedded and stored in ChromaDB
3. **Query Processing** â€“ User questions are embedded and matched against stored chunks
4. **Answer Generation** â€“ Google Gemini generates answers using only the retrieved chunks
5. **Source Display** â€“ The exact source chunks are shown alongside each answer

### Key Design Principles

- âœ… **Grounded Responses** â€“ Answers derived exclusively from document content
- âœ… **Transparent Sources** â€“ Full traceability with chunk-level references
- âœ… **Modular Architecture** â€“ Clean separation between ingestion and query logic

---

## ğŸ“ Environment Variables

### Backend (`backend/.env`)

```env
GEMINI_API_KEY=your_gemini_api_key
CHROMA_HOST=chromadb
PORT=5000
```

---

## ğŸ› ï¸ Development

### Backend Development

```bash
cd backend
npm run dev
```

### Frontend Development

```bash
cd frontend
npm run dev
```

### Build for Production

```bash
# Frontend
cd frontend
npm run build

# Backend runs via Docker in production
```

---

## ğŸ“¦ Docker Services

The `docker-compose.yml` defines two services:

- **chromadb** â€“ Vector database (port 8000)
- **backend** â€“ Node.js API server (port 5000)

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.

---
